{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "50wsyXdqEk08"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gN7Se9JnE_Yz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HgExb0HUFmF7"
   },
   "outputs": [],
   "source": [
    "def generator(z, out_dim, n_units=128, reuse=False,  alpha=0.01):    \n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(z, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(h1, alpha*h1)\n",
    "        \n",
    "        # Logits and tanh output\n",
    "        logits = tf.layers.dense(h1, out_dim, activation=None)\n",
    "        out = tf.nn.tanh(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Apfi0XTwGrbp"
   },
   "outputs": [],
   "source": [
    "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(x, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(h1, alpha*h1)\n",
    "        \n",
    "        logits = tf.layers.dense(h1, 1, activation=None)\n",
    "        out = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lHWiXGXSGw0K"
   },
   "outputs": [],
   "source": [
    "# Size of input image to discriminator\n",
    "input_size = 784 # 28x28 MNIST images flattened\n",
    "# Size of latent vector to generator\n",
    "z_size = 100\n",
    "# Sizes of hidden layers in generator and discriminator\n",
    "g_hidden_size = 128\n",
    "d_hidden_size = 128\n",
    "# Leak factor for leaky ReLU\n",
    "alpha = 0.01\n",
    "# Label smoothing \n",
    "smooth = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7vXYH8LrG8BL"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_real = tf.placeholder(tf.float32, (None, 784))#input size 28*28=784\n",
    "input_z = tf.placeholder(tf.float32, (None, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g1SreRmmFE3f"
   },
   "outputs": [],
   "source": [
    "# Generator network here\n",
    "g_model, g_logits = generator(input_z, input_size, g_hidden_size, reuse=False,  alpha=alpha)\n",
    "# g_model is the generator output\n",
    "\n",
    "# Disriminator network here\n",
    "d_model_real, d_logits_real = discriminator(input_real, d_hidden_size, reuse=False, alpha=alpha)\n",
    "d_model_fake, d_logits_fake = discriminator(g_model, d_hidden_size, reuse=True, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "N6d_BedTHRcQ"
   },
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "d_labels_real = tf.ones_like(d_logits_real) * (1 - smooth)\n",
    "d_labels_fake = tf.zeros_like(d_logits_fake)\n",
    "\n",
    "d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(labels=d_labels_real, logits=d_logits_real)\n",
    "d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(labels=d_labels_fake, logits=d_logits_fake)\n",
    "\n",
    "d_loss = tf.reduce_mean(d_loss_real + d_loss_fake)\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.ones_like(d_logits_fake), \n",
    "        logits=d_logits_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uBsJmjYvJPty"
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Get the trainable_variables, split into G and D parts\n",
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n",
    "d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer().minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1717
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 302051,
     "status": "ok",
     "timestamp": 1531158759382,
     "user": {
      "displayName": "shreyas seshadri",
      "photoUrl": "//lh5.googleusercontent.com/-ht0cS32PFig/AAAAAAAAAAI/AAAAAAAAACk/xveTa_QRfVg/s50-c-k-no/photo.jpg",
      "userId": "117283399608377626330"
     },
     "user_tz": -330
    },
    "id": "F59hMl-jJSP4",
    "outputId": "4712704b-7bcb-430e-a656-932cb37eceee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100... Discriminator Loss: 0.3603... Generator Loss: 3.9207\n",
      "Epoch 2/100... Discriminator Loss: 0.3399... Generator Loss: 4.6675\n",
      "Epoch 3/100... Discriminator Loss: 0.3664... Generator Loss: 3.5587\n",
      "Epoch 4/100... Discriminator Loss: 0.3738... Generator Loss: 3.9431\n",
      "Epoch 5/100... Discriminator Loss: 0.4238... Generator Loss: 2.9535\n",
      "Epoch 6/100... Discriminator Loss: 0.3837... Generator Loss: 3.9516\n",
      "Epoch 7/100... Discriminator Loss: 0.5606... Generator Loss: 5.1852\n",
      "Epoch 8/100... Discriminator Loss: 0.4522... Generator Loss: 3.8981\n",
      "Epoch 9/100... Discriminator Loss: 0.6053... Generator Loss: 4.2954\n",
      "Epoch 10/100... Discriminator Loss: 0.8284... Generator Loss: 2.5160\n",
      "Epoch 11/100... Discriminator Loss: 0.6784... Generator Loss: 3.6348\n",
      "Epoch 12/100... Discriminator Loss: 0.6457... Generator Loss: 3.3383\n",
      "Epoch 13/100... Discriminator Loss: 0.7339... Generator Loss: 2.5546\n",
      "Epoch 14/100... Discriminator Loss: 0.7227... Generator Loss: 3.2209\n",
      "Epoch 15/100... Discriminator Loss: 0.9960... Generator Loss: 2.6163\n",
      "Epoch 16/100... Discriminator Loss: 1.1965... Generator Loss: 1.8808\n",
      "Epoch 17/100... Discriminator Loss: 0.9048... Generator Loss: 3.1241\n",
      "Epoch 18/100... Discriminator Loss: 1.1057... Generator Loss: 2.1629\n",
      "Epoch 19/100... Discriminator Loss: 0.9123... Generator Loss: 2.1771\n",
      "Epoch 20/100... Discriminator Loss: 1.0950... Generator Loss: 1.7958\n",
      "Epoch 21/100... Discriminator Loss: 1.0205... Generator Loss: 3.5989\n",
      "Epoch 22/100... Discriminator Loss: 0.7122... Generator Loss: 2.2892\n",
      "Epoch 23/100... Discriminator Loss: 0.8401... Generator Loss: 2.1433\n",
      "Epoch 24/100... Discriminator Loss: 0.9312... Generator Loss: 2.3714\n",
      "Epoch 25/100... Discriminator Loss: 1.2219... Generator Loss: 1.7964\n",
      "Epoch 26/100... Discriminator Loss: 1.2108... Generator Loss: 1.7342\n",
      "Epoch 27/100... Discriminator Loss: 0.9961... Generator Loss: 2.3756\n",
      "Epoch 28/100... Discriminator Loss: 1.1385... Generator Loss: 1.7369\n",
      "Epoch 29/100... Discriminator Loss: 1.2778... Generator Loss: 1.1359\n",
      "Epoch 30/100... Discriminator Loss: 1.0099... Generator Loss: 2.0964\n",
      "Epoch 31/100... Discriminator Loss: 1.2765... Generator Loss: 1.9162\n",
      "Epoch 32/100... Discriminator Loss: 1.0532... Generator Loss: 1.8671\n",
      "Epoch 33/100... Discriminator Loss: 1.4336... Generator Loss: 1.3207\n",
      "Epoch 34/100... Discriminator Loss: 0.8633... Generator Loss: 2.0055\n",
      "Epoch 35/100... Discriminator Loss: 0.9108... Generator Loss: 1.9080\n",
      "Epoch 36/100... Discriminator Loss: 1.0029... Generator Loss: 1.9892\n",
      "Epoch 37/100... Discriminator Loss: 1.0298... Generator Loss: 1.3866\n",
      "Epoch 38/100... Discriminator Loss: 0.8944... Generator Loss: 1.8985\n",
      "Epoch 39/100... Discriminator Loss: 0.9739... Generator Loss: 2.1161\n",
      "Epoch 40/100... Discriminator Loss: 1.0824... Generator Loss: 1.8786\n",
      "Epoch 41/100... Discriminator Loss: 1.0939... Generator Loss: 2.4424\n",
      "Epoch 42/100... Discriminator Loss: 1.1575... Generator Loss: 1.9160\n",
      "Epoch 43/100... Discriminator Loss: 0.8119... Generator Loss: 1.9713\n",
      "Epoch 44/100... Discriminator Loss: 1.0719... Generator Loss: 1.6224\n",
      "Epoch 45/100... Discriminator Loss: 1.0138... Generator Loss: 2.1446\n",
      "Epoch 46/100... Discriminator Loss: 0.8014... Generator Loss: 2.2789\n",
      "Epoch 47/100... Discriminator Loss: 1.2193... Generator Loss: 1.3660\n",
      "Epoch 48/100... Discriminator Loss: 0.9721... Generator Loss: 1.8723\n",
      "Epoch 49/100... Discriminator Loss: 1.0609... Generator Loss: 1.5907\n",
      "Epoch 50/100... Discriminator Loss: 0.9121... Generator Loss: 1.6094\n",
      "Epoch 51/100... Discriminator Loss: 0.8744... Generator Loss: 2.0004\n",
      "Epoch 52/100... Discriminator Loss: 1.2624... Generator Loss: 1.4685\n",
      "Epoch 53/100... Discriminator Loss: 0.9146... Generator Loss: 2.0759\n",
      "Epoch 54/100... Discriminator Loss: 0.8066... Generator Loss: 2.4351\n",
      "Epoch 55/100... Discriminator Loss: 0.8850... Generator Loss: 2.1039\n",
      "Epoch 56/100... Discriminator Loss: 1.1449... Generator Loss: 1.6452\n",
      "Epoch 57/100... Discriminator Loss: 1.0643... Generator Loss: 1.8411\n",
      "Epoch 58/100... Discriminator Loss: 0.9543... Generator Loss: 1.7550\n",
      "Epoch 59/100... Discriminator Loss: 1.0918... Generator Loss: 1.7451\n",
      "Epoch 60/100... Discriminator Loss: 1.0477... Generator Loss: 1.6222\n",
      "Epoch 61/100... Discriminator Loss: 1.1670... Generator Loss: 1.4034\n",
      "Epoch 62/100... Discriminator Loss: 1.0040... Generator Loss: 1.8174\n",
      "Epoch 63/100... Discriminator Loss: 0.9087... Generator Loss: 1.7573\n",
      "Epoch 64/100... Discriminator Loss: 1.2713... Generator Loss: 1.2770\n",
      "Epoch 65/100... Discriminator Loss: 1.3464... Generator Loss: 1.6237\n",
      "Epoch 66/100... Discriminator Loss: 0.8018... Generator Loss: 2.2712\n",
      "Epoch 67/100... Discriminator Loss: 0.9606... Generator Loss: 1.5837\n",
      "Epoch 68/100... Discriminator Loss: 1.0555... Generator Loss: 1.4979\n",
      "Epoch 69/100... Discriminator Loss: 1.1093... Generator Loss: 1.9457\n",
      "Epoch 70/100... Discriminator Loss: 1.0301... Generator Loss: 1.4002\n",
      "Epoch 71/100... Discriminator Loss: 0.8333... Generator Loss: 1.8992\n",
      "Epoch 72/100... Discriminator Loss: 0.9712... Generator Loss: 1.6553\n",
      "Epoch 73/100... Discriminator Loss: 0.9963... Generator Loss: 1.5702\n",
      "Epoch 74/100... Discriminator Loss: 1.1436... Generator Loss: 1.2002\n",
      "Epoch 75/100... Discriminator Loss: 1.1726... Generator Loss: 1.2891\n",
      "Epoch 76/100... Discriminator Loss: 0.9315... Generator Loss: 1.8373\n",
      "Epoch 77/100... Discriminator Loss: 1.0464... Generator Loss: 1.4894\n",
      "Epoch 78/100... Discriminator Loss: 1.0016... Generator Loss: 1.7829\n",
      "Epoch 79/100... Discriminator Loss: 1.1276... Generator Loss: 1.3230\n",
      "Epoch 80/100... Discriminator Loss: 1.1575... Generator Loss: 1.4429\n",
      "Epoch 81/100... Discriminator Loss: 1.0325... Generator Loss: 2.0217\n",
      "Epoch 82/100... Discriminator Loss: 1.0602... Generator Loss: 1.5794\n",
      "Epoch 83/100... Discriminator Loss: 0.9856... Generator Loss: 1.5335\n",
      "Epoch 84/100... Discriminator Loss: 1.1424... Generator Loss: 1.5753\n",
      "Epoch 85/100... Discriminator Loss: 1.0386... Generator Loss: 1.4808\n",
      "Epoch 86/100... Discriminator Loss: 1.0182... Generator Loss: 2.3484\n",
      "Epoch 87/100... Discriminator Loss: 0.9893... Generator Loss: 1.7432\n",
      "Epoch 88/100... Discriminator Loss: 1.3093... Generator Loss: 1.1310\n",
      "Epoch 89/100... Discriminator Loss: 1.0566... Generator Loss: 1.3610\n",
      "Epoch 90/100... Discriminator Loss: 0.8935... Generator Loss: 1.7353\n",
      "Epoch 91/100... Discriminator Loss: 1.1268... Generator Loss: 1.8209\n",
      "Epoch 92/100... Discriminator Loss: 1.0048... Generator Loss: 1.5423\n",
      "Epoch 93/100... Discriminator Loss: 0.9339... Generator Loss: 2.0088\n",
      "Epoch 94/100... Discriminator Loss: 1.0096... Generator Loss: 1.5886\n",
      "Epoch 95/100... Discriminator Loss: 1.0263... Generator Loss: 1.7961\n",
      "Epoch 96/100... Discriminator Loss: 0.9360... Generator Loss: 1.5359\n",
      "Epoch 97/100... Discriminator Loss: 1.1283... Generator Loss: 1.3918\n",
      "Epoch 98/100... Discriminator Loss: 1.0090... Generator Loss: 1.8428\n",
      "Epoch 99/100... Discriminator Loss: 0.8279... Generator Loss: 1.8747\n",
      "Epoch 100/100... Discriminator Loss: 0.8621... Generator Loss: 2.0926\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "samples = []\n",
    "losses = []\n",
    "saver = tf.train.Saver(var_list = g_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(mnist.train.num_examples//batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Get images, reshape and rescale to pass to D\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images*2 - 1  #To make form [0,1]->[-1,1]\n",
    "            \n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            \n",
    "            # Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})\n",
    "        \n",
    "        # At the end of each epoch, get the losses and print them out\n",
    "        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})\n",
    "        train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g))    \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(\n",
    "                       generator(input_z, input_size, reuse=True),\n",
    "                       feed_dict={input_z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "GANs",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
