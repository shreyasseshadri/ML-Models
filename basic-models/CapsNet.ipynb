{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_Gd0Gczmm2mt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSk8evQkm56K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "np.random.seed(42)\n",
        "tf.set_random_seed(42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNevpHAAnCKZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "a0f61dcc-8c7c-420e-fcdb-2c13011e10cc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530965007049,
          "user_tz": -330,
          "elapsed": 8635,
          "user": {
            "displayName": "shreyas seshadri",
            "photoUrl": "//lh5.googleusercontent.com/-ht0cS32PFig/AAAAAAAAAAI/AAAAAAAAACk/xveTa_QRfVg/s50-c-k-no/photo.jpg",
            "userId": "117283399608377626330"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-4141630e56b4>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YM0AFmsRnFRp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X=tf.placeholder(shape=[None,28,28,1],dtype=tf.float32)\n",
        "#As described in the paper\n",
        "caps1_maps=32\n",
        "caps1_n_caps=6*6*caps1_maps\n",
        "caps1_dims=8\n",
        "conv1_params = {\n",
        "    \"filters\": 256,\n",
        "    \"kernel_size\": 9,\n",
        "    \"strides\": 1,\n",
        "    \"padding\": \"valid\",\n",
        "    \"activation\": tf.nn.relu,\n",
        "}\n",
        "conv2_params = {\n",
        "    \"filters\": caps1_maps * caps1_dims, \n",
        "    \"kernel_size\": 9,\n",
        "    \"strides\": 2,\n",
        "    \"padding\": \"valid\",\n",
        "    \"activation\": tf.nn.relu\n",
        "}\n",
        "#Primary Capsule layer\n",
        "conv1 = tf.layers.conv2d(X, **conv1_params)\n",
        "conv2 = tf.layers.conv2d(conv1, **conv2_params)\n",
        "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_dims])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsmLUKPynIoq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def squash(s, axis=-1, epsilon=1e-7):\n",
        "    squared_norm = tf.reduce_sum(tf.square(s), axis=axis,keep_dims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + epsilon)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = s / safe_norm\n",
        "    return squash_factor * unit_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4aUt0o9-nOJt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e40bdeab-d183-4d97-c93e-7bae839e6334",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530965011368,
          "user_tz": -330,
          "elapsed": 896,
          "user": {
            "displayName": "shreyas seshadri",
            "photoUrl": "//lh5.googleusercontent.com/-ht0cS32PFig/AAAAAAAAAAI/AAAAAAAAACk/xveTa_QRfVg/s50-c-k-no/photo.jpg",
            "userId": "117283399608377626330"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "caps1_output = squash(caps1_raw)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-97bc73aa278d>:2: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_nnMQX39nPew",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Digit Capsule Layer\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n",
        "init_sigma = 0.1\n",
        "\n",
        "W_init = tf.random_normal(\n",
        "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_dims),\n",
        "    stddev=init_sigma, dtype=tf.float32)\n",
        "W = tf.Variable(W_init)\n",
        "batch_size = tf.shape(X)[0]\n",
        "# making duplicates for each batch\n",
        "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvN1lpRdnQ_q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#expanding output of primary caps so as to get column vectors instead of scalars\n",
        "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
        "                                       name=\"caps1_output_expanded\")\n",
        "#Creating extra dim for creating vector for 10 different digit\n",
        "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
        "                                   name=\"caps1_output_tile\")\n",
        "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
        "                             name=\"caps1_output_tiled\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "REk3SC2mnSi4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled)\n",
        "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CX1f4diMnUYD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "41eabff8-49af-4bcd-be44-9d5ef20a4ba4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530965017331,
          "user_tz": -330,
          "elapsed": 1208,
          "user": {
            "displayName": "shreyas seshadri",
            "photoUrl": "//lh5.googleusercontent.com/-ht0cS32PFig/AAAAAAAAAAI/AAAAAAAAACk/xveTa_QRfVg/s50-c-k-no/photo.jpg",
            "userId": "117283399608377626330"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Round 1 of dynamic routing\n",
        "routing_weights = tf.nn.softmax(raw_weights, dim=2)\n",
        "\n",
        "weighted_predictions = tf.multiply(routing_weights, caps2_predicted)\n",
        "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True)\n",
        "caps2_output_round_1 = squash(weighted_sum, axis=-2)\n",
        "#This is done so that we can multiply for all capsule instances of i,j simultaneously\n",
        "caps2_output_round_1_tiled = tf.tile(\n",
        "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-e6649d6c3c5e>:1: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bIL4lPGTnWFD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Dot product\n",
        "agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,transpose_a=True)\n",
        "raw_weights_round_2 = tf.add(raw_weights, agreement)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WfYzHERnXmd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Round 2 Of dynamic Routing\n",
        "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
        "                                        dim=2)\n",
        "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
        "                                           caps2_predicted)\n",
        "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
        "                                     axis=1, keep_dims=True)\n",
        "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
        "                              axis=-2)\n",
        "caps2_output = caps2_output_round_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SF8rxuLQnZ7A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#For calculating prob(Norm of a vector gives prob)But directly using norm ,if it is zero causes weight problems (NaN)\n",
        "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False):\n",
        "    squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
        "                                 keep_dims=keep_dims)\n",
        "    return tf.sqrt(squared_norm + epsilon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KO47LXRJnbdA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_proba = safe_norm(caps2_output, axis=-2)\n",
        "y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srCiHfqMndFI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y = tf.placeholder(shape=[None], dtype=tf.int64)\n",
        "#For Marginal loss func L(k) = Tk max(0, (m+) − ||vk||)^2 + λ (1 − Tk) max(0, ||vk|| − (m−))^2\n",
        "m_plus = 0.9\n",
        "m_minus = 0.1\n",
        "lambda_ = 0.5\n",
        "T = tf.one_hot(y, depth=caps2_n_caps)\n",
        "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True)\n",
        "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm))\n",
        "present_error = tf.reshape(present_error_raw, shape=(-1, 10))\n",
        "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus))\n",
        "absent_error = tf.reshape(absent_error_raw, shape=(-1, 10))\n",
        "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error)\n",
        "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ES22ytVgnewa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "mask_with_labels = tf.placeholder_with_default(False, shape=())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDs5wyhLngCd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
        "                                 lambda: y,        # if True for training\n",
        "                                 lambda: y_pred)   # for testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-uiHpDanhYG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
        "                                 depth=caps2_n_caps)   # Its shape is now (?,10) but the shape of caps2_oytput is (?, 1, 10, 16, 1)\n",
        "#reshaping so we can multiply to mask\n",
        "reconstruction_mask_reshaped = tf.reshape(reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1]) \n",
        "caps2_output_masked = tf.multiply(caps2_output, reconstruction_mask_reshaped)\n",
        "#reshape operation to flatten the decoder's inputs\n",
        "decoder_input = tf.reshape(caps2_output_masked,[-1, caps2_n_caps * caps2_n_dims]) #Decoder input shape is now (?,160)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKuZlpdIni7L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "n_hidden1 = 512\n",
        "n_hidden2 = 1024\n",
        "n_output = 28 * 28\n",
        "\n",
        "hidden1 = tf.layers.dense(decoder_input, n_hidden1,activation=tf.nn.relu)\n",
        "hidden2 = tf.layers.dense(hidden1, n_hidden2,activation=tf.nn.relu)\n",
        "decoder_output = tf.layers.dense(hidden2, n_output,activation=tf.nn.sigmoid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSnF4g4unkJa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Reconstruction Loss\n",
        "X_flat = tf.reshape(X, [-1, n_output])\n",
        "squared_difference = tf.square(X_flat - decoder_output)\n",
        "reconstruction_loss = tf.reduce_mean(squared_difference)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gaZ9E4alnloa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Final loss \n",
        "alpha = 0.0005\n",
        "loss = tf.add(margin_loss, alpha * reconstruction_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d26mz1HHnoaJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "correct = tf.equal(y, y_pred, name=\"correct\")\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "#Optimizer\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "training_op = optimizer.minimize(loss)\n",
        "#init\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7FC07XwnqQZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "09295bab-4278-4948-ff0f-dab1e98ae839",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530978446533,
          "user_tz": -330,
          "elapsed": 3942958,
          "user": {
            "displayName": "shreyas seshadri",
            "photoUrl": "//lh5.googleusercontent.com/-ht0cS32PFig/AAAAAAAAAAI/AAAAAAAAACk/xveTa_QRfVg/s50-c-k-no/photo.jpg",
            "userId": "117283399608377626330"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "n_epochs = 10\n",
        "batch_size = 50\n",
        "restore_checkpoint = True\n",
        "\n",
        "n_iterations_per_epoch = mnist.train.num_examples // batch_size\n",
        "n_iterations_validation = mnist.validation.num_examples // batch_size\n",
        "best_loss_val = np.infty\n",
        "checkpoint_path = \"./my_capsule_network\"\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
        "        saver.restore(sess, checkpoint_path)\n",
        "    else:\n",
        "        init.run()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            # Run the training operation and measure the loss:\n",
        "            _, loss_train = sess.run(\n",
        "                [training_op, loss],\n",
        "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
        "                           y: y_batch,\n",
        "                           mask_with_labels: True})\n",
        "            print(\"\\r Epoch : {} Iteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
        "                      epoch,iteration, n_iterations_per_epoch,\n",
        "                      iteration * 100 / n_iterations_per_epoch,\n",
        "                      loss_train),\n",
        "                  end=\"\")\n",
        "\n",
        "        # At the end of each epoch,\n",
        "        # measure the validation loss and accuracy:\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        for iteration in range(1, n_iterations_validation + 1):\n",
        "            X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
        "            loss_val, acc_val = sess.run(\n",
        "                    [loss, accuracy],\n",
        "                    feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
        "                               y: y_batch})\n",
        "            loss_vals.append(loss_val)\n",
        "            acc_vals.append(acc_val)\n",
        "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
        "                      iteration, n_iterations_validation,\n",
        "                      iteration * 100 / n_iterations_validation),\n",
        "                  end=\" \" * 10)\n",
        "        loss_val = np.mean(loss_vals)\n",
        "        acc_val = np.mean(acc_vals)\n",
        "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
        "            epoch + 1, acc_val * 100, loss_val,\n",
        "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
        "\n",
        "        # And save the model if it improved:\n",
        "        if loss_val < best_loss_val:\n",
        "            save_path = saver.save(sess, checkpoint_path)\n",
        "            best_loss_val = loss_val\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Val accuracy: 98.8800%  Loss: 0.014695 (improved)\n",
            " Epoch : 1 Iteration: 351/1100 (31.9%)  Loss: 0.01173"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2  Val accuracy: 99.1800%  Loss: 0.010551 (improved)\n",
            " Epoch : 2 Iteration: 514/1100 (46.7%)  Loss: 0.00112"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3  Val accuracy: 99.2800%  Loss: 0.009073 (improved)\n",
            " Epoch : 3 Iteration: 590/1100 (53.6%)  Loss: 0.00653"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4  Val accuracy: 99.2400%  Loss: 0.007962 (improved)\n",
            " Epoch : 4 Iteration: 626/1100 (56.9%)  Loss: 0.00155"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5  Val accuracy: 99.4200%  Loss: 0.006943 (improved)\n",
            " Epoch : 5 Iteration: 643/1100 (58.5%)  Loss: 0.00713"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6  Val accuracy: 99.4200%  Loss: 0.007093\n",
            " Epoch : 6 Iteration: 721/1100 (65.5%)  Loss: 0.00083"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7  Val accuracy: 99.3000%  Loss: 0.006977\n",
            " Epoch : 7 Iteration: 762/1100 (69.3%)  Loss: 0.00136"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8  Val accuracy: 99.1600%  Loss: 0.008632\n",
            " Epoch : 8 Iteration: 783/1100 (71.2%)  Loss: 0.00228"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 9  Val accuracy: 99.3200%  Loss: 0.007051\n",
            " Epoch : 9 Iteration: 794/1100 (72.2%)  Loss: 0.00114"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10  Val accuracy: 99.3800%  Loss: 0.007309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u09rLgkpnr2L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c666fdf6-56e4-4fc8-b480-dc5f969e9583",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530978929952,
          "user_tz": -330,
          "elapsed": 22800,
          "user": {
            "displayName": "shreyas seshadri",
            "photoUrl": "//lh5.googleusercontent.com/-ht0cS32PFig/AAAAAAAAAAI/AAAAAAAAACk/xveTa_QRfVg/s50-c-k-no/photo.jpg",
            "userId": "117283399608377626330"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Testing the model\n",
        "n_iterations_test = mnist.test.num_examples // batch_size\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, checkpoint_path)\n",
        "    loss_tests = []\n",
        "    acc_tests = []\n",
        "    for iteration in range(1, n_iterations_test + 1):\n",
        "        X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
        "        loss_test, acc_test = sess.run(\n",
        "                [loss, accuracy],\n",
        "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
        "                           y: y_batch})\n",
        "        loss_tests.append(loss_test)\n",
        "        acc_tests.append(acc_test)\n",
        "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
        "                  iteration, n_iterations_test,\n",
        "                  iteration * 100 / n_iterations_test),\n",
        "              end=\" \" * 10)\n",
        "    loss_test = np.mean(loss_tests)\n",
        "    acc_test = np.mean(acc_tests)\n",
        "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
        "        acc_test * 100, loss_test))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_capsule_network\n",
            "Final test accuracy: 99.2900%  Loss: 0.007216\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}