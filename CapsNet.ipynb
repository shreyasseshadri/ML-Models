{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(shape=[None,28,28,1],dtype=tf.float32)\n",
    "#As described in the paper\n",
    "caps1_maps=32\n",
    "caps1_n_caps=6*6*caps1_maps\n",
    "caps1_dims=8\n",
    "conv1_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "conv2_params = {\n",
    "    \"filters\": caps1_maps * caps1_dims, \n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu\n",
    "}\n",
    "#Primary Capsule layer\n",
    "conv1 = tf.layers.conv2d(X, **conv1_params)\n",
    "conv2 = tf.layers.conv2d(conv1, **conv2_params)\n",
    "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_dims])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s, axis=-1, epsilon=1e-7):\n",
    "    squared_norm = tf.reduce_sum(tf.square(s), axis=axis,keep_dims=True)\n",
    "    safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "    squash_factor = squared_norm / (1. + squared_norm)\n",
    "    unit_vector = s / safe_norm\n",
    "    return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps1_output = squash(caps1_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Digit Capsule Layer\n",
    "caps2_n_caps = 10\n",
    "caps2_n_dims = 16\n",
    "init_sigma = 0.1\n",
    "\n",
    "W_init = tf.random_normal(\n",
    "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_dims),\n",
    "    stddev=init_sigma, dtype=tf.float32)\n",
    "W = tf.Variable(W_init)\n",
    "batch_size = tf.shape(X)[0]\n",
    "# making duplicates for each batch\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding output of primary caps so as to get column vectors instead of scalars\n",
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "#Creating extra dim for creating vector for 10 different digit\n",
    "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled)\n",
    "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 1 of dynamic routing\n",
    "routing_weights = tf.nn.softmax(raw_weights, dim=2)\n",
    "\n",
    "weighted_predictions = tf.multiply(routing_weights, caps2_predicted)\n",
    "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True)\n",
    "caps2_output_round_1 = squash(weighted_sum, axis=-2)\n",
    "#This is done so that we can multiply for all capsule instances of i,j simultaneously\n",
    "caps2_output_round_1_tiled = tf.tile(\n",
    "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dot product\n",
    "agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,transpose_a=True)\n",
    "raw_weights_round_2 = tf.add(raw_weights, agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 2 Of dynamic Routing\n",
    "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                        dim=2)\n",
    "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                           caps2_predicted)\n",
    "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                     axis=1, keep_dims=True)\n",
    "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                              axis=-2)\n",
    "caps2_output = caps2_output_round_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For calculating prob(Norm of a vector gives prob)But directly using norm ,if it is zero causes weight problems (NaN)\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False):\n",
    "    squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                 keep_dims=keep_dims)\n",
    "    return tf.sqrt(squared_norm + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = safe_norm(caps2_output, axis=-2)\n",
    "y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "#For Marginal loss func L(k) = Tk max(0, (m+) − ||vk||)^2 + λ (1 − Tk) max(0, ||vk|| − (m−))^2\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "T = tf.one_hot(y, depth=caps2_n_caps)\n",
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True)\n",
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm))\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 10))\n",
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus))\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 10))\n",
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error)\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_with_labels = tf.placeholder_with_default(False, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True for training\n",
    "                                 lambda: y_pred)   # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps)   # Its shape is now (?,10) but the shape of caps2_oytput is (?, 1, 10, 16, 1)\n",
    "#reshaping so we can multiply to mask\n",
    "reconstruction_mask_reshaped = tf.reshape(reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1]) \n",
    "caps2_output_masked = tf.multiply(caps2_output, reconstruction_mask_reshaped)\n",
    "#reshape operation to flatten the decoder's inputs\n",
    "decoder_input = tf.reshape(caps2_output_masked,[-1, caps2_n_caps * caps2_n_dims]) #Decoder input shape is now (?,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 28 * 28\n",
    "\n",
    "hidden1 = tf.layers.dense(decoder_input, n_hidden1,activation=tf.nn.relu)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2,activation=tf.nn.relu)\n",
    "decoder_output = tf.layers.dense(hidden2, n_output,activation=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction Loss\n",
    "X_flat = tf.reshape(X, [-1, n_output])\n",
    "squared_difference = tf.square(X_flat - decoder_output)\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final loss \n",
    "alpha = 0.0005\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "#Optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)\n",
    "#init\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "restore_checkpoint = True\n",
    "\n",
    "n_iterations_per_epoch = mnist.train.num_examples // batch_size\n",
    "n_iterations_validation = mnist.validation.num_examples // batch_size\n",
    "best_loss_val = np.infty\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the model\n",
    "n_iterations_test = mnist.test.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
